performance_score,mae,mse,r2,t_test_p_value,pipeline
0.00015427445763406257,0.005218708087118541,0.00015427445763406257,0.9842179062486972,0.9836549765463342,"StackingEstimator(estimator=XGBRegressor(learning_rate=0.1,max_depth=2,min_child_weight=6,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.8500000000000001,verbosity=0)),XGBRegressor(learning_rate=0.5,max_depth=4,min_child_weight=8,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.9000000000000001,verbosity=0))"
0.00013195805547391864,0.004124367349002916,0.00013195805547391864,0.9865008476797316,0.8987221505768446,"PolynomialFeatures(degree=2,include_bias=False,interaction_only=False),XGBRegressor(learning_rate=0.1,max_depth=9,min_child_weight=4,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=1.0,verbosity=0))"
0.00010845826319081358,0.004770175021834229,0.00010845826319081358,0.9889048485145803,0.9088234903046797,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.00017116013382175067exported_pipeline = XGBRegressor(learning_rate=0.1,max_depth=7,min_child_weight=5,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.35000000000000003,verbosity=0)"
0.00024087336373671686,0.005131615938674617,0.00024087336373671686,0.9753589410263777,0.9247176392786625,"StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.7000000000000001,tol=0.01)),DecisionTreeRegressor(max_depth=8,min_samples_leaf=1,min_samples_split=12))"
0.00016363261603182194,0.0054822188439490865,0.00016363261603182194,0.9832605777612861,0.9106840567773112,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.00024738089588084483exported_pipeline = XGBRegressor(learning_rate=0.5,max_depth=4,min_child_weight=19,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.8,verbosity=0)"
0.00010076018591108593,0.004167619639376703,0.00010076018591108593,0.9896923526756489,0.9886059865284423,"as npimport pandas as pdfrom sklearn.ensemble import ExtraTreesRegressorfrom sklearn.model_selection import train_test_split# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.00019828722796469297exported_pipeline = ExtraTreesRegressor(bootstrap=False,max_features=0.6500000000000001,min_samples_leaf=1,min_samples_split=2,n_estimators=100)"
0.00013990511703049265,0.004503116591938505,0.00013990511703049265,0.9856878727229132,0.9620025228289264,"StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=True,max_features=0.8,min_samples_leaf=17,min_samples_split=7,n_estimators=100)),GradientBoostingRegressor(alpha=0.95,learning_rate=0.1,loss=""huber"",max_depth=9,max_features=0.6500000000000001,min_samples_leaf=5,min_samples_split=9,n_estimators=100,subsample=0.55))"
0.00015783499330373948,0.004871262163555039,0.00015783499330373948,0.9838536676792965,0.9344855517122701,"PolynomialFeatures(degree=2,include_bias=False,interaction_only=False),RandomForestRegressor(bootstrap=False,max_features=0.7500000000000001,min_samples_leaf=9,min_samples_split=7,n_estimators=100))"
0.00016889696785811438,0.004545793722545303,0.00016889696785811438,0.9827220408230493,0.9910323132630301,"StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=True,max_features=1.0,min_samples_leaf=3,min_samples_split=11,n_estimators=100)),XGBRegressor(learning_rate=1.0,max_depth=10,min_child_weight=14,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.7500000000000001,verbosity=0))"
0.00012187742089052579,0.004781110719280978,0.00012187742089052579,0.987532084622694,0.948783190346629,"as npimport pandas as pdfrom sklearn.ensemble import GradientBoostingRegressorfrom sklearn.model_selection import train_test_split# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.00019955209083931527exported_pipeline = GradientBoostingRegressor(alpha=0.8,learning_rate=0.1,loss=""huber"",max_depth=6,max_features=0.55,min_samples_leaf=4,min_samples_split=9,n_estimators=100,subsample=0.8)"
0.0001356601917552864,0.003724770416328209,0.0001356601917552864,0.9861221235359642,0.9666504605509053,"StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.2,tol=1e-05)),GradientBoostingRegressor(alpha=0.85,learning_rate=0.5,loss=""ls"",max_depth=9,max_features=0.7000000000000001,min_samples_leaf=1,min_samples_split=14,n_estimators=100,subsample=0.9500000000000001))"
0.00013747335408456442,0.004240956779994399,0.00013747335408456442,0.9859366391835584,0.9515933725795935,"StackingEstimator(estimator=LassoLarsCV(normalize=False)),StackingEstimator(estimator=XGBRegressor(learning_rate=0.5,max_depth=9,min_child_weight=7,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.4,verbosity=0)),RandomForestRegressor(bootstrap=True,max_features=0.9000000000000001,min_samples_leaf=18,min_samples_split=2,n_estimators=100))"
0.00010252815232194104,0.004485867693123937,0.00010252815232194104,0.989511491812009,0.9807283779982792,"StackingEstimator(estimator=XGBRegressor(learning_rate=0.1,max_depth=9,min_child_weight=8,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.45,verbosity=0)),RandomForestRegressor(bootstrap=False,max_features=0.7500000000000001,min_samples_leaf=4,min_samples_split=11,n_estimators=100))"
0.00014467405044979437,0.005385908791971783,0.00014467405044979437,0.9852000164991975,0.9453676096612403,"StackingEstimator(estimator=XGBRegressor(learning_rate=0.001,max_depth=5,min_child_weight=8,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.6500000000000001,verbosity=0)),XGBRegressor(learning_rate=0.1,max_depth=4,min_child_weight=10,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.8500000000000001,verbosity=0))"
8.867602948098146e-05,0.00352897759011974,8.867602948098146e-05,0.9909285475235199,0.977851927395629,"StackingEstimator(estimator=LassoLarsCV(normalize=True)),XGBRegressor(learning_rate=0.5,max_depth=8,min_child_weight=5,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.7000000000000001,verbosity=0))"
0.00016660710064424916,0.004759621213237304,0.00016660710064424916,0.9829562915188643,0.9990943019995449,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.00020873100124810078exported_pipeline = XGBRegressor(learning_rate=0.5,max_depth=9,min_child_weight=5,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.9500000000000001,verbosity=0)"
0.00015255732327830923,0.0059569683118363415,0.00015255732327830923,0.9843935670534842,0.20696876375018275,"as npimport pandas as pdfrom sklearn.ensemble import GradientBoostingRegressorfrom sklearn.model_selection import train_test_split# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.00022844641381223208exported_pipeline = GradientBoostingRegressor(alpha=0.8,learning_rate=0.1,loss=""quantile"",max_depth=8,max_features=0.6500000000000001,min_samples_leaf=5,min_samples_split=3,n_estimators=100,subsample=1.0)"
9.073521035651442e-05,0.003745287982048843,9.073521035651442e-05,0.9907178957660809,0.9640491356333608,"StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.30000000000000004,tol=1e-05)),XGBRegressor(learning_rate=0.1,max_depth=8,min_child_weight=18,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.8,verbosity=0))"
0.0001320907600341023,0.003913318113072575,0.0001320907600341023,0.9864872721608663,0.9738142475097511,"StackingEstimator(estimator=LassoLarsCV(normalize=True)),StackingEstimator(estimator=XGBRegressor(learning_rate=0.001,max_depth=10,min_child_weight=16,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.25,verbosity=0)),XGBRegressor(learning_rate=0.5,max_depth=8,min_child_weight=9,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.6500000000000001,verbosity=0))"
0.00012662254300757297,0.004573051988618149,0.00012662254300757297,0.9870466642669131,0.9513880334409952,"PolynomialFeatures(degree=2,include_bias=False,interaction_only=False),RandomForestRegressor(bootstrap=False,max_features=0.35000000000000003,min_samples_leaf=6,min_samples_split=3,n_estimators=100))"
