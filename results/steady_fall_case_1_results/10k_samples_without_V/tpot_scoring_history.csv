performance_score,mae,mse,r2,t_test_p_value,pipeline
0.0004719621612134928,0.00831193716332918,0.0004719621612134928,0.951718831557932,0.9671995777972151,"StackingEstimator(estimator=XGBRegressor(learning_rate=0.1,max_depth=10,min_child_weight=6,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.1,verbosity=0)),StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=True,max_features=0.8500000000000001,min_samples_leaf=4,min_samples_split=9,n_estimators=100)),ElasticNetCV(l1_ratio=0.35000000000000003,tol=1e-05))"
0.0003797213671566253,0.008053355187399717,0.0003797213671566253,0.9611549552158944,0.9965385492829335,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0004845129607759279exported_pipeline = XGBRegressor(learning_rate=0.1,max_depth=8,min_child_weight=5,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.55,verbosity=0)"
0.0005844747081306642,0.00990927655870763,0.0005844747081306642,0.940208931663443,0.8330253285520035,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0007485773974868928exported_pipeline = DecisionTreeRegressor(max_depth=10,min_samples_leaf=4,min_samples_split=15)"
0.0003850477249034413,0.008080628306430056,0.0003850477249034413,0.9606100751456458,0.8992730001449529,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0004673292301926917exported_pipeline = XGBRegressor(learning_rate=0.1,max_depth=8,min_child_weight=2,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.35000000000000003,verbosity=0)"
0.0004961083724494047,0.009727455448887532,0.0004961083724494047,0.9492487028321008,0.9954584279474886,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0006162100029513019exported_pipeline = XGBRegressor(learning_rate=0.5,max_depth=4,min_child_weight=6,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.55,verbosity=0)"
0.0003810056748671992,0.008059401876960344,0.0003810056748671992,0.9610235720627488,0.964791210377214,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.000494911443521968exported_pipeline = XGBRegressor(learning_rate=0.1,max_depth=5,min_child_weight=2,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.55,verbosity=0)"
0.00035552378752950613,0.007719066812053597,0.00035552378752950613,0.963630338867125,0.9460821329616187,"StackingEstimator(estimator=LassoLarsCV(normalize=False)),RandomForestRegressor(bootstrap=False,max_features=0.55,min_samples_leaf=4,min_samples_split=7,n_estimators=100))"
0.0003624498928054772,0.007921516005638476,0.0003624498928054772,0.9629218065250049,0.9504325489017644,"StackingEstimator(estimator=RidgeCV()),XGBRegressor(learning_rate=0.1,max_depth=5,min_child_weight=7,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.6500000000000001,verbosity=0))"
0.0004984268355256064,0.009842494601719723,0.0004984268355256064,0.9490115268135383,0.9844291623421462,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.000552279587266864exported_pipeline = XGBRegressor(learning_rate=0.5,max_depth=3,min_child_weight=6,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.55,verbosity=0)"
0.0005564849495871206,0.008677749396389451,0.0005564849495871206,0.9430722507130443,0.9273373758118881,"StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False,max_features=0.7000000000000001,min_samples_leaf=9,min_samples_split=4,n_estimators=100)),MinMaxScaler(),StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False,max_features=0.8500000000000001,min_samples_leaf=2,min_samples_split=18,n_estimators=100)),LassoLarsCV(normalize=True))"
0.0004876257103503865,0.008809509951811232,0.0004876257103503865,0.9501164690881643,0.9940697127339672,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0006111312526866919exported_pipeline = XGBRegressor(learning_rate=0.5,max_depth=8,min_child_weight=3,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.9000000000000001,verbosity=0)"
0.00039404196062876716,0.007961245424304516,0.00039404196062876716,0.9596899755153161,0.9778197348183595,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0004925592983482821exported_pipeline = XGBRegressor(learning_rate=0.1,max_depth=9,min_child_weight=8,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.7000000000000001,verbosity=0)"
0.00045861862462981374,0.008749550312433591,0.00045861862462981374,0.9530838594994795,0.9895173638230594,"VarianceThreshold(threshold=0.001),XGBRegressor(learning_rate=0.5,max_depth=5,min_child_weight=7,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.8500000000000001,verbosity=0))"
0.0005632873758422308,0.00939255643038796,0.0005632873758422308,0.9423763705878384,0.8611434103498105,"StackingEstimator(estimator=RidgeCV()),StackingEstimator(estimator=RidgeCV()),GradientBoostingRegressor(alpha=0.75,learning_rate=0.5,loss=""huber"",max_depth=7,max_features=0.05,min_samples_leaf=10,min_samples_split=3,n_estimators=100,subsample=0.8500000000000001))"
0.00039766966548551925,0.00801096656847773,0.00039766966548551925,0.9593188656178688,0.9615270001333427,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0005007953262277226exported_pipeline = XGBRegressor(learning_rate=0.1,max_depth=5,min_child_weight=4,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.8,verbosity=0)"
0.00055158090673392,0.009161656627865753,0.00055158090673392,0.9435739284713497,0.920633979043797,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.0006358791462971302exported_pipeline = XGBRegressor(learning_rate=0.5,max_depth=10,min_child_weight=3,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.9500000000000001,verbosity=0)"
0.0005441028299624391,0.008979758203777074,0.0005441028299624391,0.9443389268417661,0.9437581459226192,"StackingEstimator(estimator=RidgeCV()),XGBRegressor(learning_rate=0.5,max_depth=6,min_child_weight=9,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.6000000000000001,verbosity=0))"
0.00045880525920111853,0.008816793228128969,0.00045880525920111853,0.9530647670045407,0.9800359012177833,"StackingEstimator(estimator=LassoLarsCV(normalize=True)),GradientBoostingRegressor(alpha=0.75,learning_rate=0.5,loss=""huber"",max_depth=6,max_features=0.7500000000000001,min_samples_leaf=9,min_samples_split=14,n_estimators=100,subsample=0.55))"
0.00043047950089545575,0.008019820073867354,0.00043047950089545575,0.9559624584306683,0.9846590497404919,"PolynomialFeatures(degree=2,include_bias=False,interaction_only=False),StackingEstimator(estimator=XGBRegressor(learning_rate=0.1,max_depth=1,min_child_weight=11,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.7000000000000001,verbosity=0)),RandomForestRegressor(bootstrap=False,max_features=0.4,min_samples_leaf=12,min_samples_split=8,n_estimators=100))"
0.00045434433533742376,0.008809138544835537,0.00045434433533742376,0.9535211142166063,0.8856710239082304,"as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom xgboost import XGBRegressor# NOTE: Make sure that the outcome column is labeled 'target' in the data filetpot_data = pd.read_csv('PATH/TO/DATA/FILE',sep='COLUMN_SEPARATOR',dtype=np.float64)features = tpot_data.drop('target',axis=1)training_features,testing_features,training_target,testing_target = \            train_test_split(features,tpot_data['target'],random_state=None)# Average CV score on the training set was: -0.000576769374785164exported_pipeline = XGBRegressor(learning_rate=0.5,max_depth=5,min_child_weight=6,n_estimators=100,n_jobs=1,objective=""reg:squarederror"",subsample=0.8500000000000001,verbosity=0)"
